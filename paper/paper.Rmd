---
title: |
  Automated assessment of residual plots with computer vision models
author:
  - name: Weihao Li
    affil: a
    email: weihao.li@monash.edu
  - name: Dianne Cook
    affil: a
    email: dicook@monash.edu
  - name: Emi Tanaka
    affil: b, c
    email: emi.tanaka@anu.edu.au
  - name: Susan VanderPlas
    affil: d
    email: susan.vanderplas@unl.edu
  - name: Klaus Ackermann
    affil: a
    email: Klaus.Ackermann@monash.edu
affiliation:
  - num: a
    address: |
      Department of Econometrics and Business Statistics, Monash University, Clayton, VIC, Australia
  - num: b
    address: |
      Biological Data Science Institute, Australian National University, Acton, ACT, Australia
  - num: c
    address: |
      Research School of Finance, Actuarial Studies and Statistics, Australian National University, Acton, ACT, Australia
  - num: d
    address: |
      Department of Statistics, University of Nebraska, Lincoln, Nebraska, USA
bibliography: ref.bib
abstract: |
  TBD.
keywords: |
  TBD
header-includes: |
  \usepackage{lscape}
  \usepackage{hyperref}
  \usepackage[utf8]{inputenc}
  \def\tightlist{}
  \usepackage{setspace}
  \doublespacing
output: rticles::tf_article
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  echo = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "100%",
  fig.align = "center")
```

```{r}
# Visual inference models and p-value calculation
# remotes::install_github("TengMCing/visage)

library(tidyverse)
library(visage)
library(glue)

# To control the simulation in this file
set.seed(10086)
```


\newpage
\tableofcontents
\newpage


# Introduction

Plotting residuals is commonly regarded as a standard practice in linear regression diagnostics \citep[see][]{cook1982residuals, belsley1980regression}. This visual assessment plays a crucial role in identifying deviations from model assumptions, such as linearity, homoscedasticity, and normality. It also helps in understanding the goodness of fit and various characteristics of the model.

Generating a residual plot in most statistical software is often as straightforward as executing a line of code or clicking a button. However, accurately interpreting a residual plot can be challenging. Consider Figure \ref{fig:false-finding} as an example, the residuals display a triangular shape pointing to the left. While this might suggest heteroskedasticity, it is important to avoid over-interpreting this visual pattern. In this case, the fitted regression model is correctly specified, and the triangular shape is actually a result of the skewed distribution of the predictors, rather than indicating a flaw in the model.

A residual plot can exhibit various visual features, but it is crucial to recognize that some may arise from the characteristics of predictors and the inherent randomness of the error, rather than indicating a violation of model assumptions [@li2023plot]. The concept of visual inference, as proposed by @buja2009statistical, provides an inferential framework to assess whether residual plots indeed contain visual patterns inconsistent with the model assumptions. The fundamental idea involves testing whether the actual residual plot visually differs significantly from null plots, where null plots are plotted with residuals generated from the residual rotation distribution [@langsrud2005rotation], which is a distribution consistent with the null hypothesis $H_0$ that the linear regression model is correctly specified. Typically, the visual test is accomplished through the lineup protocol, where the real residual plot is embedded within a lineup alongside several null plots. If the real residual plot can be distinguished from the lineup, it provides evidence for rejecting $H_0$.

The practice of delivering a residual plot as a lineup is generally regarded as a valuable approach. Beyond its application in residual diagnostics, the lineup protocol has integrated into the analysis of diverse subjects. For instance, \cite{loy2013diagnostic, loy2014hlmdiag, loy2015you} illustrated its applicability in diagnosing hierarchical linear models. Additionally, @widen2016graphical demonstrated its utility in geographical research, while @krishnan2021hierarchical explored its effectiveness in forensic examinations.

However, as pointed out by @li2023plot, a primary limitation of the lineup protocol lies in its reliance on human judgments. Unlike conventional statistical tests that can be performed computationally in statistical software, the lineup protocol requires human evaluation of images. This characteristic makes it less suitable for large-scale applications, given the associated high labor costs and time requirements.

There is a substantial need to develop an approach that alleviates analysts' workload by automating repetitive tasks and providing standardized results in a controlled environment. The large-scale evaluation of lineups is impractical without the use of technology and machines.

The utilization of computers to interpret data plots has a rich history, with early efforts such as "Scagnostics" by @tukey1985computer, focusing on scatterplot diagnostics. @wilkinson2005graph expanded on this work, introducing graph theoretic scagnostics, which encompassed computable measures applied to planar proximity graphs. These measures, including, but not limited to, "Outlying", "Skinny", "Stringy", "Straight", "Monotonic", "Skewed", "Clumpy", and "Striated" aimed to characterize outliers, shape, density, trend, coherence and other characteristics of the data. While this approach has been inspiring, there is a recognition [@buja2009statistical] that it may not capture all the necessary visual features that differentiate actual residual plots from null plots. A more promising alternative entails enabling machines to learn the function for extracting visual features from residual plots. Essentially, this means empowering computers to discern the crucial visual features for residual diagnostics and determining the method to extract them. 

Modern computer vision models are well-suited for addressing this challenge. They rely on deep neural networks with convolutional layers [@fukushima1982neocognitron]. These layers leverage hierarchical patterns in data, downsizing and transforming images by summarizing information in a small space. Numerous studies have demonstrated the efficacy of convolutional layers in addressing various vision tasks, including image recognition [@rawat2017deep]. Despite the widespread use of computer vision models in fields like computer-aided diagnosis [@lee2015image], pedestrian detection [@brunetti2018computer], and facial recognition [@emami2012facial], their application in reading data plots remains limited. While some studies have explored the use of computer vision models for tasks such as reading recurrence plots for time series regression [@ojeda2020multivariate], time series classification [@chu2019automatic; @hailesilassie2019financial; @hatami2018classification; @zhang2020encoding], anomaly detection [@chen2020convolutional], and pairwise causality analysis [@singh2017deep], the application of reading residual plots with computer vision models represents a relatively new field of study.

In this paper, we develop computer vision models and integrate them into the residual plots diagnostics workflow, filling the gap of.... The paper is structured as follows: ...


```{r false-finding, fig.width = 8/5*4, fig.height = 6/4*4, fig.pos="!h", fig.cap="An example residual vs fitted values plot (red line indicates 0). The vertical spread of the data points varies with the fitted values. This often indicates the existence of heteroskedasticity."}
set.seed(10131)
ori_x <- rand_lognormal()
mod <- heter_model(b = 0, x = closed_form(~-ori_x))
ori_dat <- mod$gen(300)

ori_dat %>%
  VI_MODEL$plot(theme = theme_light(), remove_grid_line = TRUE) +
  xlab("Fitted values") +
  ylab("Residuals")
```


# Methodology

## Different possible configurations of the model formula

There are various configurations of the computer vision model that can be used to assess residual plots. We discuss these configurations below based on two key components of the model formula: the input and the output format.


### Input formats

Deep learning models are in general very sensitive to the input data. The quality and relevance of the input data greatly influence the model's capacity to generate insightful and meaningful results. There are several designs of the input format can be considered.

A straightforward design involves feeding a vector of residuals along with a vector of fitted values, essentially providing all the necessary information for creating a residuals vs fitted values plot. However, a drawback of this method is the dynamic input size, which changes based on the number of observations. For modern computer vision models implemented in mainstream software like TensorFlow [@abadi2016tensorflow], the input shape is typically fixed. One solution is to pad the input vectors with leading or trailing zeros when the input tensor expects longer vectors, but it may fail if the input vector surpasses the designed length. Another strategy is to summarize the residuals and fitted values separately using histograms and utilize the counts as the input. By controlling the number of bins in the histograms, it becomes possible to provide fixed-length input vectors. Still, since histograms only capture the marginal distribution of residuals and fitted values respectively, they can not be used to differentiate visual patterns with same marginal distributions but different joint distributions.

Another design involves using an image as input. The primary advantage of this design, as opposed to the vector format, is the availability of the existing and sophisticated image processing architectures developed over the years, such as the VGG16 architecture proposed in @simonyan2014very. These architectures can effectively capture and summarize spatial information from nearby pixels, which is less straightforward with vector input. The main considerations are the image resolution and the aesthetics of the residual plot. In general, higher resolution provides more information to the model but comes with the trade-off of increased complexity and greater difficulty in training. As for the aesthetics of the residual plot, a practical solution is to consistently present residual plots in the same style to the model. This implies that the model can not accept arbitrary images as input but requires the use of the same preprocessing pipeline to convert residuals and fitted values into a standardized-style residual plot.

Providing multiple residual plots to the model, such as a pair of plots, a triplet or a lineup is also a possible option. @chopra2005learning have shown that computer vision models designed for image comparison can assess whether a pair of images are similar or dissimilar. Applied to our specific problem, we can define null plots of a fitted regression model to be similar to each other, while considering actual residual plots to be distinct from null plots of any fitted regression model. A triplet constitutes a set of three images, denoted as $image_1$, $image_2$ and $image_3$. It is often used to predict whether $image_2$ or $image_3$ is more similar to $image_1$, proving particularly useful for establishing rankings between samples. For this setup, we can apply the same criteria to define similarity between images. However, it is important to note that these two approaches usually require additional considerations regarding the loss function and, at times, non-standard training processes due to shared weights between different convolutional blocks.

Presenting a lineup to a model aligns closely with the lineup protocol. However, as the number of residual plots in a lineup increases, the resolution of the input image grows rapidly, posing challenges in training the model. We experimented with this approach in a pilot study, but the performance of the trained model was sub-optimal.


We did not explore all the mentioned input formats due the considerable costs associated with data preparation and model training. Considering the implementation cost and the interpretability of the model, we settled on the single residual plot input format.

### Output formats

Given that the input is a single residual plot represented as a fixed-resolution image, the output from the computer vision model can take one of two forms: binary or numeric. This choice determines whether the model belongs to a classification model or a regression model. The binary outcome encoded as $0$ and $1$ could be used to represent whether the input image is a null plot, or whether the input image would be rejected in a visual test conducted by humans. Training a model following the latter option requires data from prior human subject experiments, presenting difficulties in controlling the quality of data due to variations in experimental settings across different studies. Additionally, some visual inference experiments are unrelated to linear regression models or residual plot diagnostics, resulting in a limited amount of available training data.

Alternatively, the output could be a meaningful and interpretable numerical measure useful for assessing residual plots, such as the strength of suspicious visual patterns reflecting the extent of model violations and the difficulty index for identifying whether a residual plot has no issues. However, these numeric measures are often informally used in daily communication but are not typically formalized or rigorously defined. For the purpose of training a model, this numeric measure has to be quantifiable. 

In this study, we chose to define and use a distance measure to quantify the difference between the residual plot and the null plots. @vo2016localizing have also demonstrated that defining a proper distance between images can enhance the matching accuracy in image search compared to a binary outcome model.


## Distance from the good residual plots

In a visual test, the observer will be asked to choose one or more plots that stand out as most distinct from others in a given lineup. To develop a computer vision model for assessing residual plots within the visual inference framework, it is important to precisely define a numerical measure of "difference" or "distance" between plots. This distance can take the form of a basic statistical operation on pixels, such as the sum of square differences. Alternatively, it could involve established image similarity metrics like the Structural Similarity Index Measure [@wang2004image]. The challenge lies in the fact that metrics tailored for image comparison may not be suitable for evaluating data plots, where only essential plot elements require assessment [@chowdhury2018measuring]. Furthermore, scagnostics mentioned in Section \ref{introduction} could be used to construct distance metrics for residual plots comparison, but the functional form still needs to be carefully refined to accurately reflect the extent of the violations.

### Residual distribution

The distance measure proposed in this study takes into account the fact that we tried to measure how different a residual plot is from a good residual plot, or in other words, how different a given fitted regression model is from a correctly specified model. For the classical normal linear regression model, residuals $\boldsymbol{e}$ are derived from the fitted values $\hat{\boldsymbol{y}}$ and observed values $\boldsymbol{y}$. Suppose the data generating process is known and the regression model is correctly specified, by the Frisch-Waugh-Lowell theorem [@frisch1933partial], residuals $\boldsymbol{e}$ can also be treated as random variables and written as a linear transformation of the error $\boldsymbol{\varepsilon}$ formulated as $\boldsymbol{e} = \boldsymbol{R}\boldsymbol{\varepsilon}$, where $\boldsymbol{R}=\boldsymbol{I}_n -\boldsymbol{X}(\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'$ is the residual operator and an idempotent matrix, $\boldsymbol{X}$ is the design matrix, $\boldsymbol{I}_n$ is a $n$ by $n$ identity matrix, and $n$ is the number of observations.

One of the assumptions of the classical normal linear regression model is the error $\boldsymbol{\varepsilon}$ follows a multivariate normal distribution with zero mean and constant variance, i.e., $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n,\sigma^2\boldsymbol{I}_n)$. It can be known that residuals $\boldsymbol{e}$ also follow a certain probability distribution transformed from the multivariate normal distribution, which will be denoted as $Q$. This reference distribution $Q$ summarizes what good residuals should follow given the design matrix $\boldsymbol{X}$ is known and fixed.

In ordinary least square, the minimization of the sum of square residuals implies  $\sum_{i=1}^{n} e_i = 0$, making any residual value to be a linear combination of the remaining $n - 1$ residuals. This effectively means $\text{rank}(\boldsymbol{R}) = n - 1 < n$ and $Q$ is a degenerate multivariate distribution. To capture the characteristics of $Q$, such as moments, we can simulate a large numbers of $\boldsymbol{\varepsilon}$ and transform it to $\boldsymbol{e}$ to get the empirical estimates. For simplicity, in this study, we replaced the variance-covariance matrix of residuals $\text{cov}(\boldsymbol{e}, \boldsymbol{e}) = \boldsymbol{R}\sigma^2\boldsymbol{R}' = \boldsymbol{R}\sigma^2$ with a full-rank diagonal matrix $\text{diag}(\boldsymbol{R}\sigma^2)$, where $\text{diag}(.)$ sets the non-diagonal entries of a matrix to zeros. The resulting distribution for $\boldsymbol{Q}$ is $N(\boldsymbol{0}_n, \text{diag}(\boldsymbol{R}\sigma^2))$.

Distribution $Q$ is derived from the correctly specified model. However, if the model is misspecified, then the actual distribution of residuals denoted as $P$, will be different from $Q$. For example, if the data generating process contains variables correlated with any column of $\boldsymbol{X}$ but not included in $\boldsymbol{X}$, causing an omitted variable problem, $P$ will be different from $Q$ because the residual operator obtained from the fitted regression model will not be the same as $\boldsymbol{R}$. Besides, if the $\boldsymbol{\varepsilon}$ follows a non-normal distribution such as a multivariate lognormal distribution, $P$ will usually be skewed and has a long tail. 

### Kullback-Leibler divergence of $P$ from $Q$

Define a proper distance between distributions is usually easier than define a proper distance between data plots. Given the actual residual distribution $Q$ and the reference residual distribution $P$, we used a distance measure based on Kullback-Leibler divergence [@kullback1951information] to quantify the difference between two distributions

\begin{align}
\label{eq:kl-0}
D &= \log\left(1 + D_{KL}\right), \\
\label{eq:kl-1}
D_{KL} &= \int_{\mathbb{R}^{n}}\log\frac{p(\boldsymbol{e})}{q(\boldsymbol{e})}p(\boldsymbol{e})d\boldsymbol{e},
\end{align}

\noindent where $p(.)$ is the probability density function for distribution $P$, and $q(.)$ is the probability density function for distribution $Q$.

This distance measure was first proposed in @li2023plot. It was mainly designed for measuring the effect size of non-linearity and heteroskedasticity in a residual plot. @li2023plot have showed that, for a classical normal linear regression model that omits a necessary higher-order predictors $\boldsymbol{Z}$, and incorrectly assumes $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n,\sigma^2\boldsymbol{I}_n)$ while in fact $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n, \boldsymbol{V})$ with $\boldsymbol{V}$ being an arbitrary symmetric positive semi-definite matrix, $Q$ can be represented as $N(\boldsymbol{R}\boldsymbol{Z}\boldsymbol{\beta}_z, \text{diag}(\boldsymbol{R}\boldsymbol{V}\boldsymbol{R}))$. Note that the variance-covariance matrix is replaced with the diagonal matrix to ensure it is a full-rank matrix. 

Since both $P$ and $Q$ are adjusted to be multivariate normal distributions, equation \ref{eq:kl-1} can be further expanded to

\begin{align}
\label{eq:kl-2}
D_{KL} &= \frac{1}{2}\left(\log\frac{|\text{diag}(\boldsymbol{W})|}{|\text{diag}(\boldsymbol{R}\sigma^2)|} - n + \text{tr}(\text{diag}(\boldsymbol{W})^{-1}\text{diag}(\boldsymbol{R}\sigma^2)) + \boldsymbol{\mu}_z'(\text{diag}(\boldsymbol{W}))^{-1}\boldsymbol{\mu}_z\right),
\end{align}

\noindent where $\boldsymbol{\mu}_z = \boldsymbol{R}\boldsymbol{Z}\boldsymbol{\beta}_z$, and $\boldsymbol{W} = \boldsymbol{R}\boldsymbol{V}\boldsymbol{R}$. The assumed error variance $\sigma^2$ is set to be $\text{tr}(\boldsymbol{V})/n$, which is the expectation of the estimated variance.


### Evaluation of Kullback-Leibler divergence for non-normal $P$

For non-normal error $\boldsymbol{\varepsilon}$, the actual residual distribution $P$ is unlikely to be a multivariate normal distribution. Thus, equation \ref{eq:kl-2} given in @li2023plot will not be applicable to models violating the normality assumption. 

To evaluate the Kullback-Leibler divergence of non-normal $P$ from $Q$, the fallback is to solve equation \ref{eq:kl-1} numerically. However, since $\boldsymbol{e}$ is a linear transformation of non-normal random variables, it is very common that the general form of $P$ is unknown, meaning that we can not easily compute $p(\boldsymbol{e})$ using a well-known probability density function. Additionally, even if $p(\boldsymbol{e})$ can be calculated for any $\boldsymbol{e} \in \mathbb{R}^n$, it will be very difficult to do numerical integration over the $n$ dimensional space, because $n$ could be potentially very large.   

In order to approximate $D_{KL}$ in a practically computable manner, the elements of $\boldsymbol{e}$ are assumed to be independent of each other. This assumption solves both of the issues mentioned above. First, we no longer need to integrate over $n$ random variables. The result of equation \ref{eq:kl-1} is now the sum of the Kullback-Leibler divergence evaluated for each individual residual thanks for the independence assumption. Second, it is not required to know the joint probability density $p(\boldsymbol{e})$ any more. Instead, the evaluation of Kullback-Leibler divergence for an individual residual relies on the knowledge of the marginal density $p_i(e_i)$, where $e_i$ is the $i$-th residual for $i = 1, ..., n$. This is much easier to approximate through simulation. It is also worth mentioning that this independence assumption generally will not hold, since $\text{cov}(e_i, e_j) \neq 0$ if $\boldsymbol{R}_{ij} \neq 0$ for any $1 \leq i < j \leq n$, but its existence is essential for reducing the computational cost.

Given $\boldsymbol{X}$ and $\boldsymbol{\beta}$, the algorithm for approximating equation \ref{eq:kl-1} starts from simulating $m$ sets of observed values $\boldsymbol{y}$ according to the data generating process. The observed values are stored in a matrix $\boldsymbol{A}$ with $n$ rows and $m$ columns, where each column of $\boldsymbol{A}$ is a set of observed values. Then, we can get $m$ sets of realized values of $\boldsymbol{e}$ stored in the matrix $\boldsymbol{B}$ by applying the residual operator $\boldsymbol{B} = \boldsymbol{R}\boldsymbol{A}$. Furthermore, kernel density estimation (KDE) with Gaussian kernel and optimal bandwidth selected by the Silverman's rule of thumb [@silverman2018density] is applied on each row of $B$ to estimate $p_i(e_i)$ for $i = 1, ..., n$. The KDE computation can be done by the `density` function in R. 

Since the Kullback-Leibler divergence can be viewed as the expectation of the log-likelihood ratio between distribution $P$ and distribution $Q$ evaluated on distribution $P$, we can reuse the simulated residuals in matrix $\boldsymbol{B}$ to estimate the expectation by the sample mean. With the independence assumption, for non-normal $P$, $D_{KL}$ can be approximated by

\begin{align}
\label{eq:kl-3}
D_{KL} &\approx \sum_{i = 1}^{n} \hat{D}_{KL}^{(i)}, \\
\hat{D}_{KL}^{(i)} &= \frac{1}{m}\sum_{j = 1}^{m} log\frac{\hat{p_i}(B_{ij})}{q(B_{ij})},
\end{align}

\noindent where $\hat{D}_{KL}^{(i)}$ is the estimator of the Kullback-Leibler divergence for an individual residual $e_i$, $B_{ij}$ is the $i$-th row and $j$-th column entry of the matrix $B$, $\hat{p_i}(.)$ is the kernel density estimator of $p_i(.)$, $q(.)$ is the normal density function with mean zero and an assumed variance estimated as $\widehat{\sigma^2} = \sum_{b \in vec(B)}(b - \sum_{b \in vec(B)} b/nm)^2/(nm - 1)$, and $vec(.)$ is the vectorization operator which turns a $n \times m$ matrix into a $nm \times 1$ column vector by stacking the columns of the matrix on top of each other.

### Approximation of the distance measure

In the previous sections, we have defined a distance measure given in equation \ref{eq:kl-0} for quantifying the difference between the actual residual distribution $P$ and an ideal reference distribution $Q$. You may have noticed that this distance measure can only be computed when the data generating process is known. In reality, we often have no knowledge about the data generating process, otherwise we do not need to fit a linear regression model in the first place.

We tried to train a computer vision model to approximate this distance measure with a residual plot. Let $D$ be the result of equation \ref{eq:kl-0} indicating the extent of the model violations, and our estimator $\hat{D}$ is formulated as 

\begin{equation}
\label{eq:d-approx}
\hat{D} = f_{CV}(V_{h \times w}(\boldsymbol{e}, \hat{\boldsymbol{y}})),
\end{equation}

\noindent where $V_{h \times w}(.)$ is a plotting function that saves a residuals vs fitted values plot with fixed aesthetic as an image with $h \times w$ pixels and three colour channels, $f_{CV}(.)$ is a computer vision model which takes an $h \times w$ image as input and predicts the distance in the domain $[0, +\infty)$.

With the approximated distance $\hat{D}$, we will be able to know how different the underlying distribution of the residuals is from a good residual distribution. It also provides information for the strength of the visual signal embedded in the residual plot. 

The approximated distance $\hat{D}$ is not expected to be the same as the original distance $D$. This is largely because information contained in a single residual plot is limited and it may not be able to summarise all the important characteristics of the residual distribution. For a given residual distribution $P$, we can generate many different residual plots. Some of them share similar visual patterns, but some of them could be visually very different from the rest, especially for regression models with small $n$. This suggests the error of the approximation will vary depends on whether the observed residual plot is representative or not.

## Statistical testing based on the approximated distance

### Null distribution of the approximated distance

Theoretically, the distance $D$ for a correctly specified model is $0$, because $P$ will be the same as $Q$. However, the computer vision model may not necessary predict $0$ for a null plot. Using Figure \ref{fig:false-finding} as an example, it contains a visual pattern which is an indication of heteroskedasticity. We would not expect the model to be able to magically tell if the suspicious pattern is caused by the skewed distribution of the fitted values or the existence of heteroskedasticity. Additionally, some null plots could have outliers or strong visual patterns due to randomness, and a reasonable model will try to summarise those information into the prediction, resulting in $\hat{D} > 0$.

This property is not an issue if $\hat{D} \gg 0$ for which the visual signal of the residual plot is very strong, and we usually do not need any further examination of the significance of the result. However, if the visual pattern is weak or moderate, having $\hat{D}$ will not be sufficient to determine if $H_0$ should be rejected.

To solve this issue while aligning with the principle of visual inference, $\hat{D}$ can be viewed as a test statistic. And the null distribution of this statistic can be approximated by the empirical distribution of $\hat{D}_{null}$, where $\hat{D}_{null}$ is the approximated distance for a null plot simulated from the fitted regression model. The approximation of the distribution involves applying the residual rotation technique [@buja2009statistical] on the fitted regression model to obtain null residuals. The null residuals are then used to make null plots and fed into the computer vision model to get predictions. The empirical distribution is constructed with the approximated distance.

There are two types of error associated with the approximation of the null distribution. These include the sampling error and the estimation error for model parameters. Increasing the number of null plots within the distribution approximation can effectively reduce sampling error. However, the estimation error for model parameters remains irreducible. The residual rotation technique operates under the assumption of a correct fitted model, using the estimated variance instead of the unknown true variance to rescale rotated residuals. Consequently, under $H_0$, the true distribution of $\hat{D}_{null}$ will slightly deviate from the approximated null distribution.


### Estimation of quantiles of the null distribution

Let $\hat{D}_{null}^{(i)}$ be the approximated distance of the $i$-th null plots, where $i = 1,...,n_{null}$ and $n_{null} \in \mathbb{N}^+$ is a sufficiently large number. Quantiles of the null distribution can then be estimated using the sample quantiles available in statistical software such as R. The details of the sample quantile computation can be found in @hyndman1996sample. In statistical testing, analysts often care about certain quantiles of the null distribution, such as the 90% quantile, the 95% quantile and the 99% quantile. These quantiles are used as critical values to decide if $H_0$ needs to be rejected. For example, if $\hat{D}$ is greater than and equal to the 95% sample quantile $Q_{null}(0.95)$, we could say the approximated distance for the actual residual plot is significantly different from the approximated distance for null plots with 95% significance level. Based on our experience, in order to get a stable estimate of the 95% quantile, $n_{null}$ usually needs to be at least $100$. And if the null distribution has a long tail, more null plots will be needed. Alternatively, a p-value can be used to represents the probability of observing an event equally or more extreme than the given event under $H_0$, and it can be estimated by $1/m\sum_{i=1}^{n_{null}}I\left(\hat{D}_{null}^{(i)} \geq \hat{D}\right)$.

If precision in sample quantiles is not the main priority, using a pre-calculated table of quantiles is an available option. Such a table offers pre-determined quantiles for a specified number of observations. It is generated by assessing numerous null plots derived from various simulated regression models and averaging them. Essentially, this shifts the computational burden from the user to the developer. 

### Bootstrapping the approximated distance

Bootstrap is often employed in linear regression when conducting inference for estimated parameters. It is typically done by sampling individual cases with replacement and refitting the regression model. If the observed data accurately reflects the true distribution of the population, the bootstrapped estimates can be used to measure the variability of the parameter estimate without making strong distributional assumptions about the data generating process.

Similarly, bootstrap can be applied on the approximated distance $\hat{D}$. For each refitted model $M_{boot}^{(i)}$, there will be an associated residual plot $V_{boot}^{(i)}$ which can be fed into the computer vision model to obtain $\hat{D}_{boot}^{(i)}$, where $i = 1,...,n_{boot}$, and $n_{boot}$ is the number of bootstrapped samples. If we are interested in the variation of $\hat{D}$, we can use $\hat{D}_{boot}^{(i)}$ to estimate a confidence interval. 

Alternatively, since each $M_{boot}^{(i)}$ has a set of estimated coefficients $\hat{\boldsymbol{\beta}}_{boot}^{(i)}$ and an estimated variance $\hat{\sigma^2}_{boot}^{(i)}$, a new approximated null distribution can be construed and the corresponding 95% sample quantile $Q_{boot}^{(i)}(0.95)$ can be computed. Then, if $\hat{D}_{boot}^{(i)} \geq Q_{boot}^{(i)}(0.95)$, $H_0$ will be rejected for $M_{boot}^{(i)}$. The ratio of rejected $M_{boot}^{(i)}$ among all the refitted models provides an indication of how often the assumed regression model are considered to be incorrect if the data can be obtained repetitively from the same data generating process. But this approach is computationally very expensive because it requires $n_{boot} \times n_{null}$ times of residual plot assessment. In practice, $Q_{null}(0.95)$ can be used to replace $Q_{boot}^{(i)}(0.95)$ in the computation. 

## Generation of training data

### Data generating process

While observational data is frequently employed in training models for real-world applications, the data generating process of observational data often remains unknown, making computation for our target variable $D$ unattainable. Consequently, the computer vision models developed in this study were trained using synthetic data. This approach provided us with precise label annotations. Additionally, it ensured a large and diverse training dataset, as we had control over the data generating process, and the simulation of the training data was relatively cost-effective.

We have incorporated three types of residual departures of linear regression model in the training data, including non-linearity, heteroskedasticity and non-normality. All three departures can be summarised by the data generating process formulated as

\begin{align}
\label{eq:data-sim}
\boldsymbol{y} &= \boldsymbol{1}_n + \boldsymbol{x}_1 + \beta_1\boldsymbol{x}_2 + \beta_2(\boldsymbol{z} + \beta_1\boldsymbol{w}) + \boldsymbol{k} \odot \boldsymbol{\varepsilon}, \\
\boldsymbol{z} &= \text{He}_j(g(\boldsymbol{x}_1, 2)), \\
\boldsymbol{w} &= \text{He}_j(g(\boldsymbol{x}_2, 2)), \\
\boldsymbol{k} &= \sqrt{\boldsymbol{1}_n + b(2 - |a|)(\boldsymbol{x}_1 + \beta_1\boldsymbol{x}_2 - a\boldsymbol{1}_n)^2},
\end{align}

\noindent where $\boldsymbol{y}$, $\boldsymbol{x}_1$, $\boldsymbol{x}_2$, $\boldsymbol{z}$, $\boldsymbol{w}$, $\boldsymbol{k}$ and $\boldsymbol{\varepsilon}$ are vectors of size $n$, $\boldsymbol{1}_n$ is a vector of ones of size $n$, $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$ are two independent predictors, $\text{He}_j(.)$ is the $j$th-order probabilist's Hermite polynomials [@hermite1864nouveau], the $\sqrt{(.)}$ and $(.)^2$ operators are element-wise operators, $\odot$ is the Hadamard product, and $g(., k)$ is a scaling function to enforce the support of the random vector to be $[-k, k]^n$ defined as

$$g(\boldsymbol{x}, k) = 2k \cdot \frac{\boldsymbol{x} - x_{min}\boldsymbol{1}_n}{x_{max} - x_{min}} - k\boldsymbol{1}_n,~for~k > 0,$$
\noindent where $x_{min} = \underset{i \in \{ 1,...,n\}}{min} x_i$, $x_{max} = \underset{i \in \{ 1,...,n\}}{max} x_i$ and $x_i$ is the $i$-th entry of $\boldsymbol{x}$.

```{r factor}
tibble(Factor = c("j", "a", "b", "$\\beta_1$", "$\\beta_2$", " $\\text{dist}_\\varepsilon$", "$\\text{dist}_{x1}$", "$\\text{dist}_{x2}$", "$\\sigma_{\\varepsilon}$", "$\\sigma_{X1}$", "$\\sigma_{X2}$", "n"),
       Domain = c("\\{2, 3, ..., 18\\}", "[-1, 1]", "[0, 100]", "{0, 1}", "{0, 1}", "\\{discrete, uniform, normal, lognormal\\}", "\\{discrete, uniform, normal, lognormal\\}", "\\{discrete, uniform, normal, lognormal\\}", "[0.0625, 9]", "[0.3, 0.6]", "[0.3, 0.6]", "[50, 500]")) %>%
  kableExtra::kable("latex", 
                    escape = FALSE,
                    booktabs = TRUE,
                    caption = "Factors used in the data generating process for synthetic data simulation. Factor $j$ and $a$ controls the non-linearity shape and the heteroskedasticity shape respectively. Factor $b$, $\\sigma_\\varepsilon$ and $n$ control the signal strength. Factor $\\text{dist}_\\varepsilon$, $\\text{dist}_{x1}$ and $\\text{dist}_{x2}$ specifies the distribution of $\\varepsilon$, $X_1$ and $X_2$ respectively.")
```

The residuals and fitted values of the fitted model were obtained by regressing $\boldsymbol{y}$ on $\boldsymbol{x}_1$. If $\beta_1 \neq 0$, $\boldsymbol{x}_2$ was also included in the design matrix. This data generation process was adapted from @li2023plot, where it was utilized to simulate residual plots exhibiting non-linearity and heteroskedasticity visual patterns for human subject experiments. A summary of the factors utilized in Equation \ref{eq:data-sim} is provided in Table \ref{tab:factor}.

In Equation \ref{eq:data-sim}, $\boldsymbol{z}$ and $\boldsymbol{w}$ represent higher-order terms of $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$, respectively. If $\beta_2 \neq 0$, the regression model will encounter non-linearity issues. Parameter $j$ serves as a shape parameter that controls the number of tuning points in the non-linear pattern. Typically, higher values of $j$ lead to an increase in the number of tuning points, as illustrated in Figure \ref{fig:different-j}.

```{r different-j, fig.pos="!h", fig.cap = "Non-linearity forms generated for the synthetic data simulation. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$."}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, include_x2 = FALSE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, include_x2 = FALSE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```

Additionally, Scaling factor $\boldsymbol{k}$ directly affects the error distribution and it is correlated with $\boldsymbol{x}_1$ and $\boldsymbol{x}_2$. If $b \neq 0$ and $\boldsymbol{\varepsilon} \sim N(\boldsymbol{0}_n, \sigma^2\boldsymbol{I}_n)$, the constant variance assumption will be violated. Parameter $a$ is a shape parameter controlling the location of the smallest variance in a residual plot as shown in Figure \ref{fig:different-a}.

```{r different-a, fig.width=8/5*3*2, fig.height=6/4*3*2, fig.pos="!h", fig.cap = 'Heteroskedasticity forms generated for the synthetic data simulation. Different shapes are controlled by the continuous factor $a$ between -1 and 1. For $a = -1$, the residual plot exhibits a "left-triangle" shape. And for $a = 1$, the residual plot exhibits a "right-triangle" shape. '}

set.seed(10085)

# Generate data for a = -1
dat_a_n1 <- phn_model(include_z = FALSE,
                      include_x2 = FALSE,
                      a = -1,
                      b = 100)$gen(500) %>%
  mutate(a = -1)

# Generate data for other a
map(c(-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1), function(a) {
  phn_model(include_z = FALSE,
            include_x2 = FALSE,
            a = a,
            b = 100)$gen(500) %>%
  mutate(a = a)
}) %>%
  
  # Combined with data for a = -1
  bind_rows(dat_a_n1) %>%
  mutate(a = factor(a)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~a, scales = "free", ncol = 3) +
  xlab("Fitted values") +
  ylab("Residuals")
```

```{r different-e, fig.pos="!h", fig.cap = 'Non-normality forms generated for the synthetic data simulation. Four different error distributions including discrete, lognormal, normal and uniform are considered.'}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_uniform(-1.4, 1.4))$gen(500) %>%
  mutate(e_dist = "uniform")

dat_shape_2 <- phn_model(include_z = FALSE, include_x2 = FALSE, sigma = 0.8)$gen(500) %>%
  mutate(e_dist = "normal")

dat_shape_3 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_lognormal(sigma = 0.6))$gen(500) %>%
  mutate(e_dist = "lognormal")

dat_shape_4 <- phn_model(include_z = FALSE, include_x2 = FALSE, e = rand_uniform_d(-1.4, 1.4, even = TRUE))$gen(500) %>%
  mutate(e_dist = "discrete")

# Generate data for shape 2, 3 and 4. Reuse x and e.
bind_rows(dat_shape_1, dat_shape_2, dat_shape_3, dat_shape_4) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~e_dist, scales = "free", labeller = label_parsed, ncol = 2)
```

Non-normality violations arise from specifying a non-normal distribution for $\boldsymbol{\varepsilon}$. In the synthetic data simulation, four distinct error distributions are considered, including discrete, uniform, normal, and lognormal distributions, as presented in Figure \ref{fig:different-e}. Each distribution imparts unique characteristics to the residuals. The discrete error distribution introduces discreteness in residuals, while the lognormal distribution typically yields outliers. Uniform error distribution may result in residuals filling the entire space of the residual plot. All of these distributions exhibit visual distinctions from the normal error distribution.

```{r different-j-x2, fig.pos="!h", fig.cap = "Residual plots of multiple linear regression models with non-linearity issues. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$. A second predictor $\\boldsymbol{x}_2$ is introduced to the regression model to create complex shapes."}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, include_x2 = TRUE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, include_x2 = TRUE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```

```{r different-j-heter, fig.pos="!h", fig.cap = 'Residual plots of models violating both the non-linearity and the heteroskedasticity assumptions. The 17 shapes are generated by varying the order of polynomial given by $j$ in $He_j(.)$, and the "left-triangle" shape is introduced by setting $a = -1$.'}

set.seed(10086)

# Data for shape 1
dat_shape_1 <- phn_model(j = 2, a = -1, b = 100, include_x2 = FALSE, sigma = 0.05)$gen(500) %>%
  mutate(j = 2)

# Generate data for shape 2, 3 and 4. Reuse x and e.
map_df(3:18, function(j) {
  phn_model(j = j, a = -1, b = 100, include_x2 = FALSE, sigma = 0.05)$
    gen(500, computed = select(dat_shape_1, x1, e)) %>%
  mutate(j = j)
}) %>%
  
  # Combined with data for shape 1
  bind_rows(dat_shape_1) %>%
  mutate(j = factor(j)) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~j, scales = "free", labeller = label_parsed, ncol = 5)
```


```{r different-e-heter, fig.pos="!h", fig.cap = 'Residual plots of models violating both the non-normality and the heteroskedasticity assumptions. The four shapes are generated by using four different error distributions including discrete, lognormal, normal and uniform, and the "left-triangle" shape is introduced by setting $a = -1$. '}

set.seed(10085)

# Data for shape 1
dat_shape_1 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_uniform(-1.4, 1.4))$gen(500) %>%
  mutate(e_dist = "uniform")

dat_shape_2 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, sigma = 0.8)$gen(500) %>%
  mutate(e_dist = "normal")

dat_shape_3 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_lognormal(sigma = 0.6))$gen(500) %>%
  mutate(e_dist = "lognormal")

dat_shape_4 <- phn_model(a = -1, b = 100, include_z = FALSE, include_x2 = FALSE, e = rand_uniform_d(-1.4, 1.4, even = TRUE))$gen(500) %>%
  mutate(e_dist = "discrete")

# Generate data for shape 2, 3 and 4. Reuse x and e.
bind_rows(dat_shape_1, dat_shape_2, dat_shape_3, dat_shape_4) %>%
  VI_MODEL$plot(remove_axis = TRUE, remove_grid_line = TRUE, theme = theme_light()) +
  facet_wrap(~e_dist, scales = "free", labeller = label_parsed, ncol = 2)
```

Equation \ref{eq:data-sim} accommodates the incorporation of the second predictor $\boldsymbol{x}_2$. Introducing it into the data generation process by setting $\beta_1 = 1$ significantly enhances the complexity of the shapes, as illustrated in Figure \ref{fig:different-j-x2}. In comparison to Figure \ref{fig:different-j}, Figure \ref{fig:different-j-x2} demonstrates that the non-linear shape resembles a surface rather than a single curve. This augmentation can facilitate the computer vision model in learning visual patterns from residual plots of the multiple linear regression model.

In real-world analysis, it's not uncommon to encounter instances where multiple model violations coexist. In such cases, the residual plots often exhibit a mixed pattern of visual anomalies corresponding to different types of model violations. Figure \ref{fig:different-j-heter} and \ref{fig:different-e-heter} show the visual patterns of models with multiple model violations.

### Computation of scagnostics


In Section \ref{introduction}, we mentioned that scagnostics consist of a set of manually designed visual feature extraction functions. While our computer vision model will learn its own feature extraction function during training, leveraging additional information from scagnostics can enhance the model's predictive accuracy.

For each generated residual plot, we computed four scagnostics – "Monotonic," "Sparse," "Splines," and "Striped" – using the cassowaryr R package [@mason2022cassowaryr]. These computed measures, along with the number of observations from the fitted model, were provided as the second input for the computer vision model. Although other scagnostics are informative, they are currently unavailable due to a fatal bug in the compiled C program of the interp R package [@Albrecht2023interp], which may unpredictably crash the process. For reproducibility, we excluded these scagnostics from the training data.


### Crafting a balanced training set

To train a robust computer vision model, we deliberately controlled the distribution of the target variable $D$ in the training data. We ensured that it followed a uniform distribution between $0$ and $7$. This was achieved by organizing $50$ buckets, each exclusively accepting training samples with $D$ falling within the range $[7(i - 1)/49, 7i/49)$ for $i < 50$, where $i$ represents the index of the $i$-th bucket. For the $50$-th bucket, any training samples with $D \geq 7$ were accepted.

With 80000 training images prepared, each bucket accommodated a maximum of $80000 \div 50 = 1600$ training samples. The simulator iteratively sampled parameter values from the parameter space, generated residuals and fitted values using the data generation process, computed the distance, and checked if the sample fitted within the corresponding bucket. This process continued until all buckets were filled.

Similarly, we adopted the same methodology to prepare 8000 test images for performance evaluation and model diagnostics.


## Architecture of the computer vision model

The architecture of the computer vision model is adapted from a well-established architecture known as VGG16, which has demonstrated high performance in image classification [@simonyan2014very].

The model begins with an input layer of shape $n \times h \times w \times 3$, capable of handling $n$ RGB images. This is followed by a grayscale conversion layer utilizing the luma formula under the Rec. 601 standard, which converts the color image to grayscale. Grayscale suffices for our task since data points are plotted in black. We experiment with three combinations of $h$ and $w$: $32 \times 32$, $64 \times 64$, and $128 \times 128$, aiming to achieve sufficiently high image resolution for the problem at hand.

The processed image is used as the input for the first convolutional block. The model comprises at most five consecutive convolutional blocks, mirroring the original VGG16 architecture. Within each block, there are two 2D convolutional layers followed by two activation layers, respectively. Subsequently, a 2D max-pooling layer follows the second activation layer. The 2D convolutional layer convolves the input with a fixed number of $3 \times 3$ convolution filters, while the 2D max-pooling layer downsamples the input along its spatial dimensions by taking the maximum value over a $2 \times 2$ window for each channel of the input. The activation layer employs the rectified linear unit (ReLU) activation function, a standard practice in deep learning, which introduces a non-linear transformation of the output of the 2D convolutional layer. Additionally, to regularize training, a batch normalization layer is added after each 2D convolutional layer and before the activation layer. Finally, a dropout layer is appended at the end of each convolutional block to randomly set some inputs to zero during training, further aiding in regularization.

The output of the last convolutional block is summarized by either a global max pooling layer or a global average pooling layer, resulting in a two-dimensional tensor. To leverage the information contained in scagnostics, this tensor is concatenated with an additional $n \times 5$ tensor, which contains the "Monotonic," "Sparse," "Splines," and "Striped" measures, along with the number of observations for $n$ residual plots.

The concatenated tensor is then fed into the final prediction block. This block consists of two fully-connected layers. The first layer contains at least $128$ units, followed by a dropout layer. Occasionally, a batch normalization layer is inserted between the fully-connected layer and the dropout layer for regularization purposes. The second fully-connected layer consists of only one unit, serving as the output of the model.

The model weights $\boldsymbol{\theta}$ were randomly initialized and they were optimized by the Adam optimizer with the mean square error loss function

$$\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{argmin}\frac{1}{n_{train}}\sum_{i=1}^{n_{train}}(D_i - f_{\boldsymbol{\theta}}(V_i, S_i))^2,$$

\noindent where $n_{train}$ is the number of training samples, $V_i$ is the $i$-th residual plot and $S_i$ is the additional information about the $i$-th residual plot including four scagnostics and the number of observations. 


## Training and hyperparameter tuning

To achieve an optimal deep learning model, hyperparameters like the learning rate often need to be fine-tuned using a tuner. In our study, we utilized the Bayesian optimization tuner from the KerasTuner Python library [@omalley2019kerastuner] for this purpose. A comprehensive list of hyperparameters is provided in Table \ref{tab:hyperparameter}.

The "number of base filters" determines the number of filters for the first 2D convolutional layer. In the VGG16 architecture, the number of filters for the 2D convolutional layer in a block is typically twice the number in the previous block, except for the last block, which maintains the same number of convolution filters as the previous one. This hyperparameter aids in controlling the complexity of the computer vision model. Higher numbers of base filters result in more trainable parameters. Likewise, the "number of units for the fully-connected layer" determines the complexity of the final prediction block. Increasing the number of units enhances model complexity, resulting in more trainable parameters.

The dropout rate and batch normalization are flexible hyperparameters that work in conjunction with other parameters to facilitate smooth training. A higher dropout rate is necessary when the model tends to overfit the training dataset by learning too much noise. Conversely, a lower dropout rate is preferred when the model is complex and challenging to converge. Batch normalization, on the other hand, addresses the internal covariate shift problem arising from the randomness in weight initialization and input data. It helps stabilize and accelerate the training process by normalizing the activations of each layer.

Additionally, incorporating additional inputs such as scagnostics and the number of observations can potentially enhance prediction accuracy. Therefore, we allowed the tuner to determine whether these inputs were necessary for optimal model performance.

The learning rate is a crucial hyperparameter, as it dictates the step size of the optimization algorithm. A high learning rate can help the model avoid local minima but risks overshooting and missing the global minimum. Conversely, a low learning rate smoothens the training process but makes the convergence time longer and increases the likelihood of getting trapped in local minima.

Our model was trained on the M3 high-performance computing platform, using TensorFlow and Keras. During training, 80% of the training data was utilized for actual training, while the remaining 20% was used as validation data. The Bayesian optimization tuner conducted 100 trials to identify the best hyperparameter values based on validation root mean square error. The tuner then restored the best epoch of the best model from the trials. Additionally, we applied early stopping, terminating the training process if the validation root mean square error fails to improve for 50 epochs. The maximum allowed epochs was set at 2000, although no models reached this threshold.


```{r hyperparameter}
data.frame(`Hyperparameter` = c("Number of base filters", "Dropout rate for convolutional blocks", "Batch normalization for convolutional blocks", "Type of global pooling", "Ignore additional inputs", "Number of units for the fully-connected layer", "Batch normalization for the fully-connected layer", "Dropout rate for the fully-connected layer", "Learning rate"),
           Domain = c("{4, 8, 16, 32, 64}", "[0.1, 0.6]", "{false, true}", "{max, average}", "{false, true}", "{128, 256, 512, 1024, 2048}", "{false, true}", "[0.1, 0.6]", "[1e-8, 1e-1]")) %>%
  kableExtra::kable("latex", booktabs = TRUE, caption = "Name of hyperparameters and their correspoding domain for the computer vision model.")
```


## Model evaluation methods

- RMSE for the test set
- R^2
- Mean bias deviation to understand the overall bias
- quantile loss to understand how well the model captures the entire distribution of D
- Percentage of predictions within a tolerance interval (like 0.1)

- comparison with human visual inference
- overview of the human subject experiment
- metrics
- agreement rate
- check cases that are aggreed
- check cases that are disaggreed

# Results

## Optimized hyperparameters

Based on the tuning process described in Section \ref{training-and-hyperparameter-tuning}, the optimized hyperparameter values are presented in Table \ref{tab:best-hyperparameter}. It was observable that a minimum of $32$ base filters was necessary, with the preferable choice being $64$ base filters for both the $64 \times 64$ and $128 \times 128$ models, mirroring the original VGG16 architecture. The optimized dropout rate for convolutional blocks hovered around $0.4$, and incorporating batch normalization for convolutional blocks proved beneficial for performance.

All optimized models chose to retain the additional inputs, contributing to the reduction of validation error. The number of units required for the fully-connected layer was $256$, a relatively modest number compared to the VGG16 classifier, suggesting that the problem at hand was relatively straightforward. The optimized learning rates were typically smaller than the default value recommended by Keras, which is 0.001.


```{r best-hyperparameter}
data.frame(`Hyperparameter` = c("Number of base filters", "Dropout rate for convolutional blocks", "Batch normalization for convolutional blocks", "Type of global pooling", "Ignore additional inputs", "Number of units for the fully-connected layer", "Batch normalization for the fully-connected layer", "Dropout rate for the fully-connected layer", "Learning rate"),
           `32` = c("32", "0.4", "true", "max", "false", "256", "false", "0.2", "0.0003"),
           `64` = c("64", "0.3", "true", "average", "false", "256", "true", "0.4", "0.0006"),
           `128` = c("64", "0.4", "true", "average", "false", "256", "true", "0.1", "0.0052")) %>%
  kableExtra::kable("latex", 
                    booktabs = TRUE, 
                    caption = "Hyperparameters values for the optimized computer vision models with different input sizes.", 
                    col.names = c("Hyperparameter", "$32 \\times 32$", "$64 \\times 64$", "$128 \\times 128$"),
                    escape = FALSE) %>%
  kableExtra::kable_styling(latex_options = c("scale_down"))
```


## Model performance

The training and test performance for the optimized models with three different input sizes are summarized in Table \ref{tab:performance}. Among these models, the $64 \times 64$ model and the $32 \times 32$ model consistently exhibited the best training and test performance, respectively. The mean absolute error indicated that the difference between $\hat{D}$ and $D$ was approximately $0.43$ on the test set, a negligible deviation considering the normal range of $D$ typically falls between $0$ and $7$. The high $R^2$ values also suggested that the predictions were largely linearly correlated with the target.

Figure \ref{fig:model-performance} presents a hexagonal heatmap for $D - \hat{D}$ versus $\hat{D}$. The red smoothing curves, fitted by generalized additive models [@hastie2017generalized], demonstrate that all the optimized models perform admirably on both the training and test sets. No structural issues are noticeable in Figure \ref{fig:model-performance}, but some minor issues regarding over-prediction and under-prediction are observed.

The figure highlights that most under-predictions occurred when $\hat{D} < 3$, while over-predictions occurred predominantly when $3 < \hat{D} < 6$. To comprehend the reasons behind these deviations, we conducted a meta-analysis on $D - \hat{D}$ for all factors used in the data generating process. A regression tree was constructed using the rpart R package [@Terry2022rpart]. The results of the regression tree are provided in the Appendix.

The meta-analysis revealed that most issues arose from non-linearity problems and the presence of a second predictor in the regression model. When the error distribution had a very small variance, all computer vision models tended to under-predict the distance. Conversely, when the error distribution had a large variance, all models tended to over-predict the distance. Therefore, Additionally, for input images representing null plots, it was expected that the models will over-predict the distance, as explained in Section \ref{null-distribution-of-the-approximated-distance}.

Since most of the deviation stemmed from the presence of non-linearity violations, to further investigate this, we split the test set and re-evaluated the performance, as detailed in Table \ref{tab:performance-sub}. It was evident that metrics for null plots were notably worse compared to other categories. Furthermore, residual plots solely exhibiting non-normality issues were the easiest to predict, with very low test root mean square error (RMSE) around $0.3$, very low mean absolute error (MAE) around $0.2$, and very high $R^2$ around $0.94$. Residual plots with non-linearity issues were more challenging to assess than those with heteroskedasticity or non-normality issues. Assessing residual plots with heteroskedasticity was not as difficult as assessing those with non-linearity issues. When multiple violations were introduced to a residual plot, the performance metrics typically lay between the metrics for each individual violation.

Based on the model performance metrics, we will only use the best-performing model evaluated on the test set, namely the $32 \times 32$ model, for the subsequent analysis.


```{r performance}
test_pred <- readr::read_csv(here::here("paper/data/test_pred.csv"))
train_pred <- readr::read_csv(here::here("paper/data/train_pred.csv"))
meta <- readr::read_csv(here::here("paper/data/meta.csv"))


test_summary <- test_pred %>% 
  left_join(meta) %>%
  group_by(res) %>%
  summarise(RMSE = yardstick::rmse_vec(effect_size, vss),
            R2 = yardstick::rsq_vec(effect_size, vss),
            MAE = yardstick::mae_vec(effect_size, vss),
            HUBER = yardstick::huber_loss_vec(effect_size, vss)) %>%
  mutate(type = "test")

train_summary <- train_pred %>% 
  left_join(meta) %>%
  group_by(res) %>%
  summarise(RMSE = yardstick::rmse_vec(effect_size, vss),
            R2 = yardstick::rsq_vec(effect_size, vss),
            MAE = yardstick::mae_vec(effect_size, vss),
            HUBER = yardstick::huber_loss_vec(effect_size, vss)) %>%
  mutate(type = "train")

train_summary %>%
  bind_rows(test_summary) %>%
  select(res, RMSE, R2, MAE, HUBER) %>%
  mutate(res = glue::glue("${res} \\times {res}$")) %>%
  mutate(across(RMSE:MAE, ~format(.x, digits = 3))) %>%
  mutate(across(HUBER, ~format(.x, digits = 2))) %>%
  kableExtra::kable(format = "latex", 
                    booktabs = TRUE, 
                    escape = FALSE,
                    col.names = c("", "RMSE", "$R^2$", "MAE", "Huber loss"),
                    caption = "The training and test performance of three optimized models with different input sizes. The best metrics are colored in red.") %>%
  kableExtra::pack_rows("Training set", 1, 3) %>%
  kableExtra::pack_rows("Test set", 4, 6) %>%
  kableExtra::column_spec(2:5, color = c("black", "red", "black",
                                       "red", "black", "black"))
                                         
```

```{r model-performance, fig.pos="!h", fig.cap = "Hexagonal heatmap for residuals vs predicted distance on training and test data for three optimized models with different input sizes. The red lines are smoothing curves produced by fitting gnealized additive models. The area over the zero line in light pink indicates under-prediction, and the area under the zero line in light green indicates over-prediction.", dev = 'png', dpi = 300}
model_pred <- train_pred %>% 
  left_join(meta) %>%
  mutate(type = "train") %>%
  bind_rows(test_pred %>% 
              left_join(meta) %>%
              mutate(type = "test"))

model_pred %>%
  ggplot() +
  geom_hline(yintercept = 0, alpha = 0.5) +
  # geom_point(aes(effect_size, effect_size - vss), alpha = 0.03) +
  annotate("rect", 
           ymin = min(model_pred$effect_size - model_pred$vss),
                ymax = 0,
                xmin = min(model_pred$vss),
                xmax = max(model_pred$vss),
            fill = "#66c2a5",
            alpha = 0.3) +
  geom_point(data = NULL, aes(5, 0, col = "over-prediction"), shape = 15) +
  annotate("rect", 
           ymin = 0,
                ymax = max(model_pred$effect_size - model_pred$vss),
                xmin = min(model_pred$vss),
                xmax = max(model_pred$vss),
            fill = "#fc8d62",
            alpha = 0.3) +
  geom_point(data = NULL, aes(5, 0, col = "under-prediction"), shape = 15) +
  geom_hex(aes(effect_size, effect_size - vss), bins = 25) +
  geom_smooth(aes(vss, effect_size - vss), se = FALSE, col = "#e34a33") +
  facet_grid(type ~ res) +
  coord_fixed() +
  ylab(expression(D - hat(D))) +
  xlab(expression(hat(D))) +
  theme_light() +
  scale_fill_continuous(limits = c(1, 200), trans = "log10", low = "#56B1F7", high = "#132B43") +
  scale_color_manual(values = c("under-prediction" = scales::alpha("#fc8d62", 0.3),
                                "over-prediction" = scales::alpha("#66c2a5", 0.3)),
                     breaks = c("under-prediction", "over-prediction")) +
  guides(col = guide_legend(title = NULL, override.aes = list(size = 8), order = 1))
```


```{r eval = FALSE}

 # 1) root 8000 3466.12800  0.04808905  
 #   2) e_sigma>=4.108756 3288 1146.50300 -0.09147268  
 #     4) include_non_normal=FALSE 1983  817.30850 -0.22151770  
 #       8) include_heter=FALSE 1210  416.16970 -0.39866830  
 #        16) include_z=FALSE 49   20.28332 -1.22777500 * (only non-normal and large error sigma -> large over-prediction)
 #        17) include_z=TRUE 1161  360.78130 -0.36367590 *
 #       9) include_heter=TRUE 773  303.72640  0.05578156 *
 #     5) include_non_normal=TRUE 1305  244.69960  0.10613590 *
 #   3) e_sigma< 4.108756 4712 2210.89500  0.14547420  
 #     6) include_z=FALSE 2030  568.99490 -0.04100357 *
 #     7) include_z=TRUE 2682 1517.87900  0.28661890  
 #      14) include_x2=FALSE 1432  377.35510  0.07228115 *
 #      15) include_x2=TRUE 1250  999.37090  0.53216420  
 #        30) e_sigma>=0.1149429 1202  829.80300  0.49933640 *
 #        31) e_sigma< 0.1149429 48  135.83470  1.35422700  (non-linearity, second predictor, small error sigma)
 #          62) measure_striped< 0.44 40   54.18083  0.92957180 *
 #          63) measure_striped>=0.44 8   38.37423  3.47750200 * (high discreteness -> large under-prediction)

# n= 8000 
# 
# node), split, n, deviance, yval
#       * denotes terminal node
# 
#     1) root 8000 3466.1280000  0.04808905  
#       2) e_sigma>=4.108756 3288 1146.5030000 -0.09147268  
#         4) include_non_normal=FALSE 1983  817.3085000 -0.22151770  
#           8) include_heter=FALSE 1210  416.1697000 -0.39866830  
#            16) include_z=FALSE 49   20.2833200 -1.22777500  
#              32) measure_sparse< 0.3630358 7    8.9396360 -2.12605300 * (large e_sigma, null plots, not very sparse -> large over-prediction)
#              33) measure_sparse>=0.3630358 42    4.7539680 -1.07806200 * (large e_sigma, null plots, sparse -> large over-prediction)
#            17) include_z=TRUE 1161  360.7813000 -0.36367590  
#              34) measure_splines>=0.02606246 376  134.3999000 -0.59957120  
#                68) n< 143.5 116   39.6936700 -0.84719690  
#                 136) measure_monotonic>=0.07456592 7    1.3762680 -1.55893700 * (non-linearity only, small n, large e_sigma -> large over-prediction)
#                 137) measure_monotonic< 0.07456592 109   34.5436600 -0.80148880 *
#                69) n>=143.5 260   84.4197800 -0.48909200  
#                 138) e_sigma>=7.367384 63   15.0193100 -0.79693070 *
#                 139) e_sigma< 7.367384 197   61.5210600 -0.39064610  
#                   278) include_x2=FALSE 90   19.5837000 -0.53918530 *
#                   279) include_x2=TRUE 107   38.2813500 -0.26570670  
#                     558) measure_sparse< 0.5586777 84   27.3861700 -0.37645850 *
#                     559) measure_sparse>=0.5586777 23    6.1018500  0.13877850 *
#              35) measure_splines< 0.02606246 785  195.4365000 -0.25068660  
#                70) e_sigma>=5.64113 529  100.6972000 -0.37916600  
#                 140) n< 109 41   11.8877000 -0.73213180 *
#                 141) n>=109 488   83.2723700 -0.34951110  
#                   282) e_sigma>=7.90547 154   26.5350800 -0.48055150 *
#                   283) e_sigma< 7.90547 334   52.8735800 -0.28909130 *
#                71) e_sigma< 5.64113 256   67.9628700  0.01480427  
#                 142) include_x2=FALSE 113   23.8143300 -0.15341150 *
#                 143) include_x2=TRUE 143   38.4243300  0.14773000  
#                   286) n< 72.5 7    2.6335630 -0.69382120 *
#                   287) n>=72.5 136   30.5781400  0.19104510 *
#           9) include_heter=TRUE 773  303.7264000  0.05578156  
#            18) b< 32.23487 487  221.2986000 -0.09202344  
#              36) b< 0.2107948 67   26.8944500 -0.51926710  
#                72) n< 126 7    3.3765570 -1.24628700 * (large e_sigma, heteroskedasticity, small b, small n -> over-prediction)
#                73) n>=126 60   19.3863300 -0.43444810 *
#              37) b>=0.2107948 420  180.2232000 -0.02386791  
#                74) measure_splines>=0.05482434 223   77.7820400 -0.18200180  
#                 148) b< 7.647324 119   51.8788300 -0.32137070  
#                   296) measure_sparse>=0.4117995 111   43.0150500 -0.38433220 *
#                   297) measure_sparse< 0.4117995 8    2.3184780  0.55221930 *
#                 149) b>=7.647324 104   20.9469900 -0.02253160 *
#                75) measure_splines< 0.05482434 197   90.5523700  0.15513650  
#                 150) b>=5.165768 63   26.5340300 -0.02749363  
#                   300) include_x2=FALSE 35    7.9867080 -0.19247040 *
#                   301) include_x2=TRUE 28   16.4039600  0.17872730  
#                     602) measure_monotonic< 0.03127537 21    4.3016860 -0.07172379 *
#                     603) measure_monotonic>=0.03127537 7    6.8333090  0.93008060 *
#                 151) b< 5.165768 134   60.9291300  0.24099990  
#                   302) x2_sigma< 0.5865284 126   52.7919200  0.19154170  
#                     604) x2_k>=5.5 106   43.2640200  0.11519510  
#                      1208) j< 17.5 93   38.5219100  0.05750563  
#                        2416) e_even=TRUE 41   15.2536200 -0.17697420  
#                          4832) measure_sparse>=0.5392181 29    8.8138120 -0.40043160 *
#                          4833) measure_sparse< 0.5392181 12    1.4922590  0.36304760 *
#                        2417) e_even=FALSE 52   19.2367200  0.24238400  
#                          4834) measure_splines>=0.01044483 31    7.8888290 -0.02089042 *
#                          4835) measure_splines< 0.01044483 21    6.0272570  0.63102720 *
#                      1209) j>=17.5 13    2.2184070  0.52789690 *
#                     605) x2_k< 5.5 20    5.6354220  0.59617870 *
#                   303) x2_sigma>=0.5865284 8    2.9747040  1.01996600 * (large e_sigma, small spline dependence, heteroskedasticity, large x2_sigma -> large under-prediction)
#            19) b>=32.23487 286   53.6723400  0.30746350  
#              38) include_x2=FALSE 139   17.0423600  0.13425010 *
#              39) include_x2=TRUE 147   28.5161400  0.47125030 *
#         5) include_non_normal=TRUE 1305  244.6996000  0.10613590  
#          10) measure_splines>=0.08262098 455   47.2275500  0.00622856 *
#          11) measure_splines< 0.08262098 850  190.4994000  0.15961570  
#            22) include_heter=FALSE 594   22.8029300  0.08838938 *
#            23) include_heter=TRUE 256  157.6908000  0.32488300  
#              46) measure_striped>=0.47 185   31.8984100  0.17734340 *
#              47) measure_striped< 0.47 71  111.2723000  0.70931710  
#                94) e_even=FALSE 32   18.0084200  0.22023240  
#                 188) a>=-0.205314 21    5.0433870 -0.06070074 *
#                 189) a< -0.205314 11    8.1435320  0.75655920 *
#                95) e_even=TRUE 39   79.3286800  1.11061700  
#                 190) b< 2.373839 11    0.2423938  0.06215229 *
#                 191) b>=2.373839 28   62.2437600  1.52251400  (discrete e_dist,  high discreteness, heteroskedasticity, small splines dependence, non-normality, large e_sigma -> large under-prediction)
#                   382) x2_dist=even_discrete,lognormal,uniform 19   33.0782400  1.01336800 *
#                   383) x2_dist=normal 9   13.8421700  2.59737800 *
#       3) e_sigma< 4.108756 4712 2210.8950000  0.14547420  
#         6) include_z=FALSE 2030  568.9949000 -0.04100357  
#          12) b< 28.58967 1159  316.0764000 -0.12131010  
#            24) include_non_normal=FALSE 441  185.6341000 -0.33971870  
#              48) include_heter=FALSE 60    8.3705760 -1.15981800 * (null plots, small e_sigma -> large over-prediction)
#              49) include_heter=TRUE 381  130.5548000 -0.21056910  
#                98) b< 0.4619795 85   22.8319800 -0.60512210 *
#                99) b>=0.4619795 296   90.6909300 -0.09726844  
#                 198) b>=1.080411 252   72.3919200 -0.15604160  
#                   396) b< 2.878015 65   20.2765800 -0.37135770 *
#                   397) b>=2.878015 187   48.0544100 -0.08119905 *
#                 199) b< 1.080411 44   12.4430700  0.23934130 *
#            25) include_non_normal=TRUE 718   96.4846700  0.01283772 *
#          13) b>=28.58967 871  235.4979000  0.06585671  
#            26) include_heter=FALSE 361  111.6977000 -0.10528680  
#              52) include_non_normal=FALSE 46    6.2078660 -1.13547200 * (null plots, small e_sigma -> large over-prediction)
#              53) include_non_normal=TRUE 315   49.5417500  0.04515286  
#               106) e_sigma>=0.2273127 247   17.7293100 -0.01172324 *
#               107) e_sigma< 0.2273127 68   28.1111100  0.25174690  
#                 214) measure_splines>=0.03710735 23   10.7097800 -0.09471551 *
#                 215) measure_splines< 0.03710735 45   13.2294000  0.42882770 *
#            27) include_heter=TRUE 510  105.7420000  0.18699950  
#              54) include_non_normal=TRUE 235   41.6282300  0.05417806 *
#              55) include_non_normal=FALSE 275   56.4252500  0.30050140  
#               110) include_x2=FALSE 137   20.9155600  0.17983570 *
#               111) include_x2=TRUE 138   31.5346400  0.42029280  
#                 222) b< 70.08912 96   17.2702500  0.30834030 *
#                 223) b>=70.08912 42   10.3110100  0.67618420 *
#         7) include_z=TRUE 2682 1517.8790000  0.28661890  
#          14) include_x2=FALSE 1432  377.3551000  0.07228115  
#            28) measure_splines>=0.07058104 731  178.2000000 -0.01483915  
#              56) e_sigma>=2.44726 138   40.9281500 -0.18500620  
#               112) include_heter=FALSE 59   25.9010700 -0.41304000  
#                 224) include_non_normal=FALSE 38   17.2724700 -0.68141390  
#                   448) measure_splines>=0.1003984 20    5.6757540 -1.07654300 * (non-linearity only, moderate e_sigma -> large over-prediction)
#                   449) measure_splines< 0.1003984 18    5.0046810 -0.24238140 *
#                 225) include_non_normal=TRUE 21    0.9391262  0.07258882 *
#               113) include_heter=TRUE 79    9.6678490 -0.01470237 *
#              57) e_sigma< 2.44726 593  132.3458000  0.02476126  
#               114) e_sigma>=0.158351 527  105.6393000 -0.00147316 *
#               115) e_sigma< 0.158351 66   23.4476700  0.23423920  
#                 230) b< 71.04535 55   15.9627800  0.12578350 *
#                 231) b>=71.04535 11    3.6032230  0.77651720 *
#            29) measure_splines< 0.07058104 701  187.8212000  0.16312990  
#              58) n< 62.5 33   14.3260500 -0.31035730 *
#              59) n>=62.5 668  165.7314000  0.18652070 *
#          15) include_x2=TRUE 1250  999.3709000  0.53216420  
#            30) e_sigma>=0.1149429 1202  829.8030000  0.49933640  
#              60) e_sigma>=2.906419 328  129.1838000  0.27288400  
#               120) measure_splines>=0.04413018 134   50.5691600  0.09851024  
#                 240) b< 2.910823 23   10.1825400 -0.20213530  
#                   480) x2_k>=6.5 15    4.3945260 -0.49662390 *
#                   481) x2_k< 6.5 8    2.0480610  0.35003080 *
#                 241) b>=2.910823 111   37.8769400  0.16080620  
#                   482) b>=7.320158 97   22.8753700  0.08661750 *
#                   483) b< 7.320158 14   10.7686400  0.67482760 *
#               121) measure_splines< 0.04413018 194   71.7259100  0.39332780  
#                 242) j< 7.5 60   32.0807400  0.19368700  
#                   484) x2_sigma< 0.343438 10   12.3504400 -0.51629950 *
#                   485) x2_sigma>=0.343438 50   13.6813400  0.33568430 *
#                 243) j>=7.5 134   36.1830100  0.48271920 *
#              61) e_sigma< 2.906419 874  677.4868000  0.58432080  
#               122) measure_splines>=0.04519141 613  424.1886000  0.45960850  
#                 244) n>=202.5 393  209.4163000  0.32432050  
#                   488) include_non_normal=TRUE 188   90.6250100  0.18780810  
#                     976) x2_k< 7.5 101   41.5156400  0.04431936 *
#                     977) x2_k>=7.5 87   44.6157600  0.35438700  
#                      1954) measure_sparse>=0.391553 80   35.8617500  0.29057300 *
#                      1955) measure_sparse< 0.391553 7    4.7050420  1.08369000 * (not sparse, non-normality, large n, small e_sigma, second predictor, non-linearity -> large under-prediction)
#                   489) include_non_normal=FALSE 205  112.0748000  0.44951240  
#                     978) e_sigma>=0.2441489 175   86.5106600  0.36295000  
#                      1956) measure_splines>=0.07729252 127   55.6660800  0.25807550 *
#                      1957) measure_splines< 0.07729252 48   25.7519700  0.64043040  
#                        3914) b>=21.95702 24    7.5694370  0.34280130 *
#                        3915) b< 21.95702 24   13.9305500  0.93805950  
#                          7830) x2_dist=even_discrete,uniform 13    2.6412500  0.51209270 *
#                          7831) x2_dist=lognormal,normal 11    6.1427810  1.44147500 *
#                     979) e_sigma< 0.2441489 30   16.6036900  0.95445990  
#                      1958) a< -0.6962934 8    1.2723410  0.16916930 *
#                      1959) a>=-0.6962934 22    8.6039200  1.24002000 *
#                 245) n< 202.5 220  194.7300000  0.70128210  
#                   490) measure_sparse>=0.6256282 81   38.6326300  0.36079080  
#                     980) measure_monotonic>=0.237124 8    0.2987567 -0.26782100 *
#                     981) measure_monotonic< 0.237124 73   34.8262100  0.42967980 *
#                   491) measure_sparse< 0.6256282 139  141.2344000  0.89969790  
#                     982) n>=97 88   72.2102700  0.72131320  
#                      1964) x1_sigma>=0.4241299 54   26.0940300  0.51943470  
#                        3928) e_sigma>=0.2485052 45   19.5310800  0.39828410  
#                          7856) x1_sigma< 0.5588507 38   11.8880800  0.26476900 *
#                          7857) x1_sigma>=0.5588507 7    3.2882920  1.12308000 *
#                        3929) e_sigma< 0.2485052 9    2.6000360  1.12518800 *
#                      1965) x1_sigma< 0.4241299 34   40.4201400  1.04194400  
#                        3930) j< 7.5 11    8.6545230  0.32377180 *
#                        3931) j>=7.5 23   23.3787300  1.38541700  
#                          7862) b< 20.93247 14   10.8261000  0.95400130 *
#                          7863) b>=20.93247 9    5.8936770  2.05650900 * (moderate b, large j, small x1_sigma, small n, small sparse, second predictor, non-linearity -> large under-prediction)
#                     983) n< 97 51   61.3921000  1.20749900  
#                      1966) e_dist=uniform 16    6.5588030  0.49547860 *
#                      1967) e_dist=even_discrete,lognormal 35   43.0135900  1.53299400  
#                        3934) measure_monotonic>=0.04508442 17   19.7133200  0.99850550 *
#                        3935) measure_monotonic< 0.04508442 18   13.8570300  2.03778800 *
#               123) measure_splines< 0.04519141 261  221.3718000  0.87722750  
#                 246) measure_sparse>=0.654522 53   26.6382500  0.36906170  
#                   492) measure_striped>=0.57 11    2.8015390 -0.20941000 *
#                   493) measure_striped< 0.57 42   19.1917400  0.52056620 *
#                 247) measure_sparse< 0.654522 208  177.5598000  1.00671200  
#                   494) e_sigma>=2.064604 95   37.3057400  0.73978960  
#                     988) n< 94.5 10    2.7534790  0.01814227 *
#                     989) n>=94.5 85   28.7318400  0.82468930  
#                      1978) measure_sparse< 0.3468051 11    2.5461540  0.27928850 *
#                      1979) measure_sparse>=0.3468051 74   22.4272100  0.90576240 *
#                   495) e_sigma< 2.064604 113  127.7952000  1.23111600  
#                     990) measure_splines>=0.03040418 40   30.6303500  0.78917000  
#                      1980) include_non_normal=TRUE 16    4.6253120  0.29159560 *
#                      1981) include_non_normal=FALSE 24   19.4029000  1.12088600 *
#                     991) measure_splines< 0.03040418 73   85.0712900  1.47327800  
#                      1982) b< 90.99624 66   59.3920200  1.35710600  
#                        3964) a>=-0.6695046 52   41.5200500  1.19796400  
#                          7928) x2_sigma< 0.4287885 20   11.8256700  0.80120150  
#                           15856) measure_splines>=0.009178431 7    2.5265790  0.21136300 *
#                           15857) measure_splines< 0.009178431 13    5.5523770  1.11880700 *
#                          7929) x2_sigma>=0.4287885 32   24.5782200  1.44594000  
#                           15858) x2_sigma>=0.4851646 24   12.4345200  1.18424600  
#                             31716) e_dist=uniform 7    0.5259639  0.41482310 *
#                             31717) e_dist=even_discrete,lognormal 17    6.0580960  1.50106700 *
#                           15859) x2_sigma< 0.4851646 8    5.5692240  2.23102400 * (small x2_sigma, small b, small splines dependence, small e_sigma, small sparse, second predictor, non-linearity -> large under-prediction)
#                        3965) a< -0.6695046 14   11.6634200  1.94820600 *
#                      1983) b>=90.99624 7   16.3901800  2.56861600 * (large b, small splines dependence, small e_sigma, small sparse, second predictor, non-linearity -> large under-prediction)
#            31) e_sigma< 0.1149429 48  135.8347000  1.35422700  
#              62) measure_striped< 0.44 40   54.1808300  0.92957180  
#               124) include_non_normal=TRUE 21   14.0872600  0.39910760  
#                 248) include_heter=TRUE 13    5.7917480  0.06683663 *
#                 249) include_heter=FALSE 8    4.5279800  0.93904800 *
#               125) include_non_normal=FALSE 19   27.6530600  1.51587400 *
#              63) measure_striped>=0.44 8   38.3742300  3.47750200 * (high discreteness, small e_sigma, second predictor, non-linearity -> large under-prediction)

model_pred %>%
  filter(res == 32L) %>%
  filter(type == "test") %>%
  mutate(.resid = effect_size - vss) %>%
  mutate(include_z = factor(include_z),
         include_heter = factor(include_heter),
         include_x2 = factor(include_x2),
         include_non_normal = factor(include_non_normal),
         e_even = factor(e_even),
         x1_even = factor(x1_even),
         x2_even = factor(x2_even)) %>%
  select(-filename, -vss, -res, -plot_uid, -data_type, -effect_size, -type) %>%
  rpart::rpart(.resid ~ ., data = ., control = rpart::rpart.control(cp = 0.001), model = TRUE) -> test_tree
```


```{r performance-sub}

model_pred %>%
  filter(res == 32L) %>%
  group_by(type, include_non_normal, include_heter, include_z) %>%
  summarise(n = n(), 
            RMSE = yardstick::rmse_vec(effect_size, vss),
            R2 = yardstick::rsq_vec(effect_size, vss),
            MAE = yardstick::mae_vec(effect_size, vss),
            HUBER = yardstick::huber_loss_vec(effect_size, vss)) %>%
  mutate(violations = ifelse(include_z, "non-linearity", "null")) %>%
  mutate(violations = ifelse(include_heter, glue("{violations} + heteroskedasticity"), violations)) %>%
  mutate(violations = ifelse(include_non_normal, glue("{violations} + non-normality"), violations)) %>%
  mutate(violations = gsub("null \\+ ", "", violations)) %>%
  ungroup() %>%
  select(type, violations, n, RMSE, R2, MAE, HUBER) %>%
  arrange(desc(type)) %>%
  select(-type) %>%
  mutate(n = format(n)) %>%
  mutate(across(RMSE:MAE, ~format(.x, digits = 3))) %>%
  mutate(across(HUBER, ~format(.x, digits = 2))) %>%
  mutate(violations = ifelse(violations == "null", "no violations", violations)) %>%
  mutate(R2 = gsub(".*NA.*", "", R2)) %>%
  kableExtra::kable("latex", 
                    booktabs = TRUE,
                    escape = FALSE,
                    col.names = c("Violations", "\\#samples", "RMSE", "$R^2$", "MAE", "Huber loss"),
                    caption = "The training and test performance of the $32 \\times 32$ model presented with different model violations.  The best metrics are colored in red.") %>%
  kableExtra::kable_styling(latex_options = c("scale_down")) %>%
  kableExtra::pack_rows("Training set", 1, 8) %>%
  kableExtra::pack_rows("Test set", 9, 16) %>%
  kableExtra::column_spec(3, color = c(rep("black", 4), "red", rep("black", 3),
                                       rep("black", 4), "red", rep("black", 3))) %>%
  kableExtra::column_spec(4, color = c(rep("black", 4), "red", rep("black", 3),
                                       rep("black", 4), "red", rep("black", 3))) %>%
  kableExtra::column_spec(5, color = c(rep("black", 4), "red", rep("black", 3),
                                       rep("black", 4), "red", rep("black", 3))) %>%
  kableExtra::column_spec(6, color = c(rep("black", 4), "red", rep("black", 3),
                                       rep("black", 4), "red", rep("black", 3)))
```

## Comparison with human visual inference

### Overview of the human subject experiment

In order to check the validity of the proposed computer vision model, residual plots presented in the human subject experiment conducted by @li2023plot will be assessed. This study has collected $7974$ human responses to $1152$ lineups. Each lineup contains one randomly placed actual residual plot and 19 null plots. Among the $1152$ lineups, $24$ are attention check lineups in which the visual patterns are designed to be extremely obvious and very different from the corresponding to null plots, $36$ are null lineups where all the lineups consist of only null plots, $279$ are lineups with uniform predictor distribution evaluated by $11$ participants, and the remaining $813$ are lineups with discrete, skewed or normal predictor distribution evaluated by $5$ participants. Attention check lineups and null lineups will not be assessed in the following analysis. 

In @li2023plot, the residual plots are simulated from a data generating process which is a special case of Equation \ref{eq:data-sim}. The main characteristic is the model violations are introduced separately, meaning non-linearity and heteroskedasticity will not co-exist in one lineup but assigned uniformly to all lineups. Additionally, non-normality and multiple predictors are not considered in the experimental design.


### Model performance on the human data

```{r rd-human}
vss_32 <- readRDS(here::here("paper/data/vss_32.rds"))

experiment <- vi_survey %>%
  group_by(unique_lineup_id) %>%
  summarise(across(everything(), first)) %>%
  select(unique_lineup_id, attention_check, null_lineup, prop_detect,
         answer, effect_size, conventional_p_value,
         p_value, type, shape, a, b, x_dist, e_dist,
         e_sigma, include_z, k, n)

experiment <- vss_32 %>%
  left_join(experiment) %>%
  mutate(conventional_reject = conventional_p_value <= 0.05) %>%
  mutate(reject = p_value <= 0.05) %>%
  mutate(model_reject = vss_p_value <= 0.05) %>%
  mutate(model_boot_reject = vss_boot_p_value <= 0.05)

experiment <- experiment %>%
  left_join(readRDS(here::here("paper/data/actual_ss.rds")))
```

```{r experiment-performance}
experiment %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  group_by(type) %>%
  summarise(RMSE = yardstick::rmse_vec(actual_ss, vss),
            R2 = yardstick::rsq_vec(actual_ss, vss),
            MAE = yardstick::mae_vec(actual_ss, vss),
            HUBER = yardstick::huber_loss_vec(actual_ss, vss)) %>%
  mutate(across(RMSE:HUBER, ~format(.x, digits = 3))) %>%
  kableExtra::kable(format = "latex", 
                    booktabs = TRUE, 
                    escape = FALSE,
                    col.names = c("Voilation", "RMSE", "$R^2$", "MAE", "Huber loss"),
                    caption = "The performance of the $32 \\times 32$ model on the data used in the human subject experiment.")
```

For each lineup used in @li2023plot, there is one actual residual plot and 19 null plots. While the distance $D$ for the actual residual plot depends on the underlying data generating process, the distance $D$ for the null plots is zero. We have used our optimized computer vision model to predict $\hat{D}$ for both the actual residual plots and the null plots. 

The performance metrics of $\hat{D}$ for actual residual plots are outlined in Table \ref{tab:experiment-performance}. It's notable that all performance metrics are slightly worse than those evaluated on the test data. Nevertheless, the mean absolute error remains at a low level, and the linear correlation between the prediction and the true value remains very high. Lineups with non-linearity issues are more challenging to predict than those with heteroskedasticity issues.

For null plots, a histogram of the predictions is provided in Figure \ref{fig:hist-null-human}. In most cases, $\hat{D}$ is predicted to be around $1$. As $\hat{D}$ increases, the number of predictions decreases. The 90% and 95% sample quantiles are approximately at $2$, and the 99% sample quantile is around $2.5$. These sample quantiles can serve as global critical values for determining whether a residual plot needs to be rejected. We have also computed the 95% sample quantiles by assessing 200 null plots for each fitted model used in the experiment. The difference between decisions based on these two types of critical values is minor, with decisions aligning `r experiment %>% filter(!attention_check) %>% filter(!null_lineup) %>% summarise(q = mean(model_reject == (2.03 <= vss)) * 100) %>% pull(q) %>% format(digits = 3)`% of the time. Therefore, we will be using the 95% sample quantiles obtained from each fitted model in the following analysis. 

Figure \ref{fig:human-mosaic} and Table \ref{tab:human-table} provide insights into the agreement between decisions made by the computer vision model and visual tests conducted by humans. It can be observed that visual tests are less sensitive than the computer vision model, as they reject almost only when the computer vision model does. The agreement rates between visual tests and the computer vision model are 67.96% and 66.84% for residual plots containing heteroskedasticity and non-linearity patterns, respectively.

When plotting the decision against the distance, as depicted in Figure \ref{fig:power}, we observe that for the computer vision model, when the distance $D > 3$, almost all residual plots will be rejected. However, for visual tests conducted by humans, the threshold is higher at $D > 5$. Visual tests also demonstrate low sensitivity to residual plots with small distances compared to the computer vision model.

Figure \ref{fig:delta} displays the scatter plot of the weighted detection rate vs the $\delta$-difference. In the experiment conducted in @li2023plot, participants were allowed to make multiple selections for a lineup. The weighted detection rate is computed by assigning weights to each detection. If the participant selects zero plots, the corresponding weight is 0.05; otherwise, the weight is 1 divided by the number of selections. The $\delta$-difference is defined by @chowdhury2018measuring as

\begin{equation}
\delta = \hat{D} - \underset{j}{max}\left(\hat{D}_{null}^{(j)}\right) \quad \text{for}~j = 1,...,m-1,
\end{equation}

\noindent where $\hat{D}_{null}^{(j)}$ is the approximated distance for the $j$-th null plot, and $m$ is the number of plots in a lineup.

Figure \ref{fig:delta} indicates that the weighted detection rate increases as the $\delta$-difference increases, particularly when the $\delta$-difference is greater than zero. A negative $\delta$-difference suggests that there is at least one null plot in the lineup with a stronger visual signal than the actual residual plot. In some instances, the weighted detection rate is close to one, yet the $\delta$-difference is negative. This discrepancy implies that the distance measure, or the approximated distance, may not perfectly reflect actual human behavior.


```{r hist-null-human, fig.pos =  "!h", fig.cap = "Histogram of predicted distance on null plots used in the human subject experiment. Sample quantiles are drawn in different colors."}
lineup_vss <- readRDS(here::here("paper/data/lineup_vss.rds"))
lineup_vss <- lineup_vss %>%
  group_by(unique_lineup_id) %>%
  mutate(delta_diff = vss[!null] - max(vss[null])) %>%
  mutate(gamma_diff = sum(vss[null] > vss[!null]))

lineup_vss %>%
  filter(null) %>%
  left_join(vi_survey %>%
              group_by(unique_lineup_id) %>% 
              summarise(type = first(type), 
                        attention_check = first(attention_check),
                        null_lineup = first(null_lineup))) %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  ggplot() +
  geom_histogram(aes(vss)) +
  geom_vline(aes(xintercept = quantile(vss, 0.9), col = "Q(0.90)")) +
  geom_vline(aes(xintercept = quantile(vss, 0.95), col = "Q(0.95)")) +
  geom_vline(aes(xintercept = quantile(vss, 0.99), col = "Q(0.99)")) +
  xlab(expression(hat(D))) +
  theme_light() +
  facet_wrap(~type) +
  labs(col = "")
```

```{r cache = TRUE}
vi_lineup <- get_vi_lineup()
```


```{r human-table}
experiment %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  group_by(type) %>%
  summarise(n = format(n()), agree = format(sum(model_reject == reject)), rate = format(mean(model_reject == reject), digits = 4)) %>%
  kableExtra::kable("latex", 
                    booktabs = TRUE,
                    escape = FALSE,
                    col.names = c("Violations", "\\#Samples", "\\#Agreements", "Agreement rate"),
                    caption = "Summary of decisions made by the visual test conducted by human and decisions made by the computer vision model.")
```



```{r human-mosaic, fig.pos = "!h", fig.cap = "Rejection rate ($p$-value $\\leq0.05$) of visual test conditional on the computer vision model decision on non-linearity (left) and heteroskedasticity (right) lineups displayed using a mosaic plot. The visual test rejects less frequently than the computer vision model, and (almost) only rejects when the computer vision model does."}
library(ggmosaic)

experiment %>%
  filter(!attention_check) %>%
  filter(!null_lineup) %>%
  mutate(type = ifelse(type == "polynomial", "non-linearity", type)) %>%
  mutate(type = ifelse(type == "non-linearity", "Non-linearity", "Heteroskedasticity")) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity"))) %>%
  mutate(model_reject = ifelse(model_reject, "Reject", "No")) %>%
  mutate(reject = ifelse(reject, "Reject", "No")) %>%
  mutate(across(c(model_reject, reject), ~factor(.x, levels = c("Reject", "No")))) %>%
  ggplot() +
  geom_mosaic(aes(x = ggmosaic::product(reject, model_reject), 
                  fill = reject)) +
  facet_grid(~type) +
  ylab("Visual tests reject") +
  xlab("Computer vision model rejects") +
  # labs(fill = "Visual tests reject") +
  scale_fill_brewer("", palette = "Dark2") +
  theme_bw() +
  theme(legend.position = "none") +
  coord_fixed()
```



```{r power, fig.pos = "!h", fig.cap = "Comparison of decisions made by visual tests and the computer vision model. The data points are fitted with logistic regression models with no intercept but an offset equals to $log(0.05/0.95)$."}

max_actual_ss <- max(filter(experiment, !attention_check, !null_lineup)$actual_ss)

model_glm_pred_poly <- glm(model_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Non-linearity", name = "Computer vision model", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

visual_glm_pred_poly <- glm(reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Non-linearity", name = "Visual test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

conv_glm_pred_poly <- glm(conventional_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type == "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Non-linearity", name = "conventional", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

model_glm_pred_heter <- glm(model_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Heteroskedasticity", name = "Computer vision model", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

visual_glm_pred_heter <- glm(reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Heteroskedasticity", name = "Visual test", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

conv_glm_pred_heter <- glm(conventional_reject ~ actual_ss - 1, 
    data = filter(experiment, !attention_check, !null_lineup, type != "polynomial") %>% mutate(offset0 = log(0.05/0.95)), 
    offset = offset0,
    family = binomial()) %>%
  predict(data.frame(actual_ss = seq(0, max_actual_ss, 0.01), offset0 = log(0.05/0.95)),
          type = "response") %>%
  data.frame(d = seq(0, max_actual_ss, 0.01), type = "Heteroskedasticity", name = "conventional", value = .) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity")))

mutate(experiment, type = ifelse(type == "polynomial", "Non-linearity", "Heteroskedasticity")) %>%
  mutate(type = factor(type, levels = c("Non-linearity", "Heteroskedasticity"))) %>%
  filter(!null_lineup, !attention_check) %>%
  pivot_longer(c(model_reject, reject)) %>%
  mutate(name = ifelse(name == "model_reject", "Computer vision model", "Visual test")) %>%
  ggplot() +
  ggbeeswarm::geom_quasirandom(aes(actual_ss, value), size = 0.5) +
  geom_line(data = model_glm_pred_poly, aes(d, value + 1, col = "Computer vision model"), linewidth = 0.8) +
  geom_line(data = model_glm_pred_heter, aes(d, value + 1, col = "Computer vision model"), linewidth = 0.8) +
  geom_line(data = visual_glm_pred_poly, aes(d, value + 1, col = "Visual test"), linewidth = 0.8) +
  geom_line(data = visual_glm_pred_heter, aes(d, value + 1, col = "Visual test"), linewidth = 0.8) +
  geom_line(data = mutate(model_glm_pred_poly, name = "Visual test"), aes(d, value + 1, col = "Computer vision model"), linetype = 2, linewidth = 0.8) +
  geom_line(data = mutate(model_glm_pred_heter, name = "Visual test"), aes(d, value + 1, col = "Computer vision model"), linetype = 2, linewidth = 0.8) +
  geom_line(data = mutate(visual_glm_pred_poly, name = "Computer vision model"), aes(d, value + 1, col = "Visual test"), linetype = 2, linewidth = 0.8) +
  geom_line(data = mutate(visual_glm_pred_heter, name = "Computer vision model"), aes(d, value + 1, col = "Visual test"), linetype = 2, linewidth = 0.8) +
  facet_grid(name ~ type) +
  theme_light() +
  scale_color_brewer("", palette = "Dark2") +
  theme(legend.position = "bottom") +
  xlab("D") +
  ylab("Reject")


```


```{r delta, fig.pos = "!h", fig.cap = "A weighted detection rate vs $\\delta$-differnence plot."}
experiment %>%
  left_join(lineup_vss %>%
              group_by(unique_lineup_id) %>%
              summarise(delta_diff = first(delta_diff), gamma_diff = first(gamma_diff))) %>%
  ggplot() +
  geom_point(aes(delta_diff, prop_detect), alpha = 0.3) +
  geom_smooth(aes(delta_diff, prop_detect), se = FALSE) +
  ylab("Weighted detection rate") +
  xlab(expression(delta-difference)) +
  theme_light()
```


```{r fig.pos = "!h", fig.cap = "A weighted detection rate vs $\\gamma$-number of extreme nulls plot.", eval = FALSE}
experiment %>%
  left_join(lineup_vss %>%
              group_by(unique_lineup_id) %>%
              summarise(delta_diff = first(delta_diff), gamma_diff = first(gamma_diff))) %>%
  ggplot() +
  geom_point(aes(gamma_diff, prop_detect)) +
  ylab("Weighted detection rate") +
  xlab(expression(gamma-number~of~extreme~nulls)) +
  theme_light()
```


## Data examples

### Boston Housing

```{r fig.pos = "!h", fig.cap = "Residual plot of the regression model fitted on the Boston housing data."}

housing <- read_csv(here::here("paper/data/housing.csv"))
mod <- lm(MEDV ~ ., data = housing)

my_vi <- autovi::auto_vi(fitted_mod = mod)
```

```{r}
if (!file.exists(here::here("paper/data/boston_check.rds"))) {
  my_vi$keras_mod <- keras::load_model_tf(here::here("paper/data/phn_v2_32.keras"))
  my_vi$check(null_draws = 200L, boot_draws = 200L)
  saveRDS(my_vi$check_result, here::here("paper/data/boston_check.rds"))
}

```

```{r}
my_vi$check_result <- readRDS(here::here("paper/data/boston_check.rds"))
```


```{r fig.pos = "!h"}
p1 <- my_vi$plot_resid()
p2 <- my_vi$summary_plot() +
  ggtitle("", subtitle = "") +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "")

patchwork::wrap_plots(p1, p2)
```


### Insurance

```{r fig.pos = "!h", fig.cap = "Residual plot of the regression model fitted on the Boston housing data."}
insurance <- read_csv(here::here("paper/data/insurance.csv"))
mod <- lm(charges ~ ., data = insurance)


my_vi <- autovi::auto_vi(fitted_mod = mod)
```

```{r}
if (!file.exists(here::here("paper/data/insurance_check.rds"))) {
  my_vi$keras_mod <- keras::load_model_tf(here::here("paper/data/phn_v2_32.keras"))
  my_vi$check(null_draws = 200L, boot_draws = 200L)
  saveRDS(my_vi$check_result, here::here("paper/data/insurance_check.rds"))
}

```

```{r}
my_vi$check_result <- readRDS(here::here("paper/data/insurance_check.rds"))
```


```{r fig.pos = "!h"}
p1 <- my_vi$plot_resid()
p2 <- my_vi$summary_plot() +
  ggtitle("", subtitle = "") +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "")

patchwork::wrap_plots(p1, p2)
```

### Wine Quality

```{r}
wine <- read_csv(here::here("paper/data/WineQT.csv"))
mod <- lm(quality ~ ., data = select(wine, -Id))

my_vi <- autovi::auto_vi(fitted_mod = mod)
```


```{r}
if (!file.exists(here::here("paper/data/wine_check.rds"))) {
  my_vi$keras_mod <- keras::load_model_tf(here::here("paper/data/phn_v2_32.keras"))
  my_vi$check(null_draws = 200L, boot_draws = 200L)
  saveRDS(my_vi$check_result, here::here("paper/data/wine_check.rds"))
}
```

```{r}
my_vi$check_result <- readRDS(here::here("paper/data/wine_check.rds"))
```


```{r fig.pos = "!h"}
p1 <- my_vi$plot_resid()
p2 <- my_vi$summary_plot() +
  ggtitle("", subtitle = "") +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "")

patchwork::wrap_plots(p1, p2)
```

### Datasaurus

```{r}
dino <- datasauRus::datasaurus_dozen %>% filter(dataset == "dino")
mod <- lm(y ~ ., data = select(dino, -dataset))

my_vi <- autovi::auto_vi(fitted_mod = mod)
```

```{r}
if (!file.exists(here::here("paper/data/dino_check.rds"))) {
  my_vi$keras_mod <- keras::load_model_tf(here::here("paper/data/phn_v2_32.keras"))
  my_vi$check(null_draws = 200L, boot_draws = 200L)
  saveRDS(my_vi$check_result, here::here("paper/data/dino_check.rds"))
}
```

```{r}
my_vi$check_result <- readRDS(here::here("paper/data/dino_check.rds"))
```

```{r fig.pos = "!h"}
p1 <- my_vi$plot_resid()
p2 <- my_vi$summary_plot() +
  ggtitle("", subtitle = "") +
  theme(legend.position = "bottom", legend.box = "vertical") +
  labs(linetype = "")

patchwork::wrap_plots(p1, p2)
```

<!-- ## When the model works -->

<!-- - simple examples (non-linearity, heteroskedasticity, ...) -->
<!-- - datasaurus -->

<!-- ## When the model does not work -->

<!-- - human detect but model does not -->
<!-- - cartoon residuals? -->


<!-- ## Workflow: how one use this model? (small showcase)  -->

<!-- # Dicussion -->

<!-- There are other kinds of residual departures like autocorrelation that are not considered in this study. The primary goal of this study is to establish a new way of evaluating residual plot and conducting visual test with computer vision models. Building computer vision models for other model violations and other types of diagnostic plots could be future directions of this field.    -->

# Conclusion

- Summary of findings
- Contributions to the field
- Future directions for research

# Acknowledgements
